{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "from tqdm import tqdm\n",
    "from gym import wrappers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def running_average(x, window_size, mode='valid'):\n",
    "    return np.convolve(x, np.ones(window_size) / window_size, mode=mode).max()\n",
    "\n",
    "def check_solution(env, policy, n_episodes = 100, max_steps = 100, to_wrap = False, to_send = False, name2save = ''):\n",
    "    ns = env.observation_space.n\n",
    "    na = env.action_space.n\n",
    "    count_dones = np.zeros(n_episodes)\n",
    "    count_steps = np.zeros(n_episodes)\n",
    "    if to_wrap:\n",
    "        env = wrappers.Monitor(env, name2save)\n",
    "        \n",
    "    for i in range(n_episodes):\n",
    "        observation = env.reset() \n",
    "        for step in range(max_steps): \n",
    "            action = policy[observation].argmax()\n",
    "            observation, reward, done, info = env.step(action)\n",
    "            count_dones[i] += reward\n",
    "            count_steps[i] += 1\n",
    "            if done:\n",
    "                break\n",
    "    \n",
    "    env.close()\n",
    "    if to_wrap and to_send:\n",
    "        gym.upload(name2save, api_key='sk_bExD4VfCSQukGlQkYKBhdQ')\n",
    "    \n",
    "    return running_average(count_dones, 100).max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def select_a_with_epsilon_greedy(curr_s, q_value, epsilon=0.1):\n",
    "    a = np.argmax(q_value[curr_s, :])\n",
    "    if np.random.rand() < epsilon:\n",
    "        a = np.random.randint(q_value.shape[1])\n",
    "    return a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_greedy_policy(Q):\n",
    "    ns, na = Q.shape\n",
    "    policy = np.zeros((ns, na))\n",
    "    best_actions = Q.argmax(axis = 1)\n",
    "    policy[np.arange(ns), best_actions] = 1 \n",
    "    return policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sarsa(env, n_episodes = 1, eps = 1.0, eps_decay = 0.999,\n",
    "          gamma = 0.99, alpha = 0.1, kappa = 0.01, max_steps = 100, print_options = True):\n",
    "    eps *= 1.0\n",
    "    alpha *= 1.0\n",
    "    \n",
    "    ns = env.observation_space.n\n",
    "    na = env.action_space.n\n",
    "\n",
    "    #terminal_states = find_terminal_states(env)\n",
    "    Q = np.zeros((ns, na))\n",
    "    \"\"\" \n",
    "    Q = np.random.normal(scale = 0.01, size = ns * na).reshape((ns, na))\n",
    "    for s in range(ns):\n",
    "        if terminal_states[s] == 1:\n",
    "            Q[s] = np.zeros(na)\n",
    "    \"\"\"\n",
    "    \n",
    "    avg_reward = None\n",
    "\n",
    "    for i in range(n_episodes):\n",
    "        if i % 1000 == 0 and print_options:\n",
    "            print(i)\n",
    "        s = env.reset()\n",
    "        a = np.random.choice(na, 1, p = policy[s])[0]\n",
    "        for step in range(max_steps):\n",
    "            new_s, reward, done, info = env.step(a)\n",
    "            #new_a = np.random.choice(na, 1, p = policy[new_s])[0]\n",
    "            new_a = select_a_with_epsilon_greedy(new_s, Q, epsilon = eps)\n",
    "            \n",
    "            if done:\n",
    "                # Running average of the terminal reward, which is used for controlling an exploration rate\n",
    "                # (This idea of controlling exploration rate by the terminal reward is suggested by JKCooper2)\n",
    "                # See https://gym.openai.com/evaluations/eval_xSOlwrBsQDqUW7y6lJOevQ\n",
    "                if avg_reward == None:\n",
    "                    avg_reward = reward\n",
    "                else:\n",
    "                    avg_reward = kappa * reward + (1 - kappa) * avg_reward\n",
    "                if reward > avg_reward:\n",
    "                    # Bias the current policy toward exploitation\n",
    "                    eps *= eps_decay\n",
    "                    \n",
    "            Q[s][a] += alpha  * (reward + gamma * Q[new_s][new_a] - Q[s][a])\n",
    "            s = new_s\n",
    "            a = new_a\n",
    "            if done:\n",
    "                break\n",
    "    return get_eps_greedy_policy(Q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sarsa_lambda(env, n_episodes = 1000, gamma = 0.99, alpha = 0.1, lambda_coef = 0.0, \n",
    "                 eps = 0.1, eps_decay = 0.995, kappa = 0.01, max_steps = 100):\n",
    "    ns = env.observation_space.n\n",
    "    na = env.action_space.n\n",
    "    \n",
    "    E = np.zeros((ns, na))\n",
    "    Q = np.zeros((ns, na))\n",
    "    avg_reward = None\n",
    "    for i in range(n_episodes):\n",
    "        s = env.reset()\n",
    "        a = np.random.choice(na)\n",
    "        for t in range(max_steps):\n",
    "            new_s, reward, done, info = env.step(a)\n",
    "            new_a = select_a_with_epsilon_greedy(new_s, Q, epsilon = eps)\n",
    "            delta = reward + gamma * Q[new_s][new_a] - Q[s][a]\n",
    "            E[s][a] += 1\n",
    "            Q += alpha * delta * E\n",
    "            E *= (gamma * lambda_coef)\n",
    "            s = new_s\n",
    "            a = new_a\n",
    "            \n",
    "            if done:\n",
    "                # Running average of the terminal reward, which is used for controlling an exploration rate\n",
    "                # (This idea of controlling exploration rate by the terminal reward is suggested by JKCooper2)\n",
    "                # See https://gym.openai.com/evaluations/eval_xSOlwrBsQDqUW7y6lJOevQ\n",
    "                kappa = 0.01\n",
    "                if avg_reward == None:\n",
    "                    avg_reward = reward\n",
    "                else:\n",
    "                    avg_reward = kappa * reward + (1 - kappa) * avg_reward\n",
    "                if reward > avg_reward:\n",
    "                    # Bias the current policy toward exploitation\n",
    "                    eps *= eps_decay\n",
    "            \n",
    "            if done:\n",
    "                break\n",
    "    policy = get_greedy_policy(Q)\n",
    "    return policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import itertools\n",
    "def find_sarsa_hyperparams(env):\n",
    "    alphas = [0.01, 0.1, 0.5]\n",
    "    eps = [0.5, 0.7, 1.0]\n",
    "    kappas = [0.001, 0.01, 0.1]\n",
    "    n_episodes = [20000]\n",
    "    eps_decays = [1.0, 0.995]\n",
    "    \n",
    "    best_score = 0\n",
    "    history = []\n",
    "    for (alpha, eps,eps_decay, kappa, n) in itertools.product(alphas, eps, eps_decays, kappas, n_episodes):\n",
    "        policy = sarsa_lambda(env, n_episodes = n, eps = eps, eps_decay = eps_decay,  \n",
    "                              alpha = alpha, kappa = kappa, lambda_coef = 0)\n",
    "        score = check_solution(env,policy, n_episodes=250)\n",
    "        history.append((alpha, eps,eps_decay, kappa, n, score))\n",
    "        if score > best_score:\n",
    "            best_score = score\n",
    "            best_params = (alpha, eps,eps_decay, kappa, n)\n",
    "            \n",
    "    print(best_params)\n",
    "    return history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Frozer Lake 8x8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-07-03 16:55:00,372] Making new env: FrozenLake8x8-v0\n"
     ]
    }
   ],
   "source": [
    "name = 'FrozenLake8x8-v0'\n",
    "env = gym.make(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n",
      "0.1\n",
      "0.2\n",
      "0.3\n",
      "0.4\n",
      "0.5\n",
      "0.6\n",
      "0.7\n",
      "0.8\n",
      "0.9\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "scores = []\n",
    "for lambda_coef in np.linspace(0,1,11):\n",
    "    print(lambda_coef)\n",
    "    np.random.seed(42)\n",
    "    policy_lambda = sarsa_lambda(env, n_episodes = 20000, \n",
    "                                 eps = 0.7, eps_decay = 0.995,\n",
    "                                 alpha = 0.1, lambda_coef = lambda_coef)\n",
    "    score = check_solution(env, policy_lambda, n_episodes = 1000, max_steps = 250)\n",
    "    scores.append(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "best_lambda = np.argmax(scores) * 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.20000000000000001"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_lambda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "policy__lake_lambda = sarsa_lambda(env, n_episodes = 20000, \n",
    "                                 eps = 0.7, eps_decay = 0.995,\n",
    "                                 alpha = 0.1, lambda_coef = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.92999999999999994"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "check_solution(env, policy__lake_lambda, n_episodes = 1000, max_steps = 250)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.95999999999999996"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "check_solution(env, policy__lake_lambda, n_episodes = 1000, max_steps = 250)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Лучший средний результат 0.9 для $\\lambda$=0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Taxi-v1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-06-30 20:42:10,275] Making new env: Taxi-v1\n"
     ]
    }
   ],
   "source": [
    "name = 'Taxi-v1'\n",
    "env_taxi = gym.make(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.1, 0.7, 0.995, 0.1, 20000)\n"
     ]
    }
   ],
   "source": [
    "history = find_sarsa_hyperparams(env_taxi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n",
      "0.1\n",
      "0.2\n",
      "0.3\n",
      "0.4\n",
      "0.5\n",
      "0.6\n",
      "0.7\n",
      "0.8\n",
      "0.9\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "scores = []\n",
    "for lambda_coef in np.linspace(0,1,11):\n",
    "    print(lambda_coef)\n",
    "    np.random.seed(42)\n",
    "    policy_lambda = sarsa_lambda(env, n_episodes = 20000, \n",
    "                                 eps = 0.7, eps_decay = 0.995, kappa = 0.1,\n",
    "                                 alpha = 0.1, lambda_coef = lambda_coef)\n",
    "    score = check_solution(env, policy_lambda, n_episodes = 1000, max_steps = 250)\n",
    "    scores.append(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "best_lambda = np.argmax(scores) * 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[11.15,\n",
       " 11.16,\n",
       " 10.720000000000001,\n",
       " 11.06,\n",
       " 11.24,\n",
       " 11.16,\n",
       " 10.82,\n",
       " 10.81,\n",
       " 10.970000000000002,\n",
       " 10.99,\n",
       " -1038.29]"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "policy_taxi_lambda = sarsa_lambda(env_taxi, n_episodes = 20000, \n",
    "                                 eps = 0.7, eps_decay = 0.995,\n",
    "                                 alpha = 0.1, lambda_coef = best_lambda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11.08"
      ]
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "check_solution(env_taxi, policy_taxi_lambda, n_episodes = 1000, max_steps = 250)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.40000000000000002"
      ]
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_lambda"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Лучший средний результат 11.08 для $\\lambda$=0.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
