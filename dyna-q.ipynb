{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from gym import wrappers\n",
    "def running_average(x, window_size, mode='valid'):\n",
    "    return np.convolve(x, np.ones(window_size) / window_size, mode=mode).max()\n",
    "\n",
    "def check_solution(env, policy, n_episodes = 100, max_steps = 100, to_wrap = False, to_send = False, name2save = ''):\n",
    "    ns = env.observation_space.n\n",
    "    na = env.action_space.n\n",
    "    count_dones = np.zeros(n_episodes)\n",
    "    count_steps = np.zeros(n_episodes)\n",
    "    if to_wrap:\n",
    "        env = wrappers.Monitor(env, name2save)\n",
    "        \n",
    "    for i in range(n_episodes):\n",
    "        observation = env.reset() \n",
    "        for step in range(max_steps): \n",
    "            action = policy[observation].argmax()\n",
    "            observation, reward, done, info = env.step(action)\n",
    "            count_dones[i] += reward\n",
    "            count_steps[i] += 1\n",
    "            if done:\n",
    "                break\n",
    "    \n",
    "\n",
    "    env.close()\n",
    "    if to_wrap and to_send:\n",
    "        gym.upload(name2save, api_key='sk_bExD4VfCSQukGlQkYKBhdQ')\n",
    "        \n",
    "    return running_average(count_dones, 100).max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def select_a_with_epsilon_greedy(curr_s, q_value, epsilon=0.1):\n",
    "    a = np.argmax(q_value[curr_s, :])\n",
    "    if np.random.rand() < epsilon:\n",
    "        a = np.random.randint(q_value.shape[1])\n",
    "    return a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_greedy_policy(Q):\n",
    "    ns, na = Q.shape\n",
    "    policy = np.zeros((ns, na))\n",
    "    best_actions = Q.argmax(axis = 1)\n",
    "    policy[np.arange(ns), best_actions] = 1 \n",
    "    return policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def dyna_q(env, n_episodes = 1000, n_repeats = 10, gamma = 0.999, alpha = 0.1, eps = 0.9, eps_decay = 0.995, kappa = 0.01):\n",
    "    max_steps = env.spec.tags.get('wrapper_config.TimeLimit.max_episode_steps')\n",
    "    \n",
    "    ns = env.observation_space.n\n",
    "    na = env.action_space.n\n",
    "    \n",
    "    Q = np.zeros((ns, na))\n",
    "    R = np.zeros((ns, na))\n",
    "    S = np.zeros((ns, na))\n",
    "    \n",
    "    \n",
    "    history_length = np.zeros(n_episodes)\n",
    "    history_reward = np.zeros(n_episodes)\n",
    "    avg_reward = None\n",
    "    visited = {}\n",
    "    for i_episode in range(n_episodes):\n",
    "        s = env.reset()\n",
    "        for t in range(max_steps):\n",
    "            a = select_a_with_epsilon_greedy(s, Q, eps)\n",
    "            \n",
    "            if s not in visited.keys():\n",
    "                visited[s] = set()\n",
    "            visited[s].add(a)\n",
    "\n",
    "            \n",
    "            new_s, reward, done, info = env.step(a)\n",
    "            new_a = Q[new_s].argmax()\n",
    "            Q[s][a] += alpha * (reward + gamma * Q[new_s, new_a] - Q[s][a])\n",
    "            \n",
    "            history_length[i_episode] = t\n",
    "            history_reward[i_episode] += reward * gamma ** t\n",
    "            \n",
    "            for repeat in range(n_repeats):\n",
    "                s = np.random.choice(list(visited.keys()))\n",
    "                a = np.random.choice(tuple(visited[s]))\n",
    "                reward = R[s, a]\n",
    "                s1  = S[s, a]\n",
    "                new_a = Q[s1].argmax()\n",
    "                Q[s][a] += alpha * (reward + gamma * Q[s1, new_a] - Q[s][a])\n",
    "                \n",
    "            s = new_s\n",
    "            \n",
    "            #eps *= eps_decay\n",
    "            if done:\n",
    "                # Running average of the terminal reward, which is used for controlling an exploration rate\n",
    "                # (This idea of controlling exploration rate by the terminal reward is suggested by JKCooper2)\n",
    "                # See https://gym.openai.com/evaluations/eval_xSOlwrBsQDqUW7y6lJOevQ\n",
    "                if avg_reward == None:\n",
    "                    avg_reward = reward\n",
    "                else:\n",
    "                    avg_reward = kappa * reward + (1 - kappa) * avg_reward\n",
    "                if reward > avg_reward:\n",
    "                    # Bias the current policy toward exploitation\n",
    "                    eps *= eps_decay\n",
    "            if done:\n",
    "                break\n",
    "    return history_length, get_greedy_policy(Q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 402,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def q_learning(env, n_episodes = 1000, gamma = 0.999, alpha = 0.1, eps = 0.9, eps_decay = 0.995, kappa = 0.01):\n",
    "    max_steps = env.spec.tags.get('wrapper_config.TimeLimit.max_episode_steps')\n",
    "    \n",
    "    ns = env.observation_space.n\n",
    "    na = env.action_space.n\n",
    "    \n",
    "    Q = np.zeros((ns, na))\n",
    "    \n",
    "    history_length = np.zeros(n_episodes)\n",
    "    history_reward = np.zeros(n_episodes)\n",
    "    avg_reward = None\n",
    "    for i_episode in range(n_episodes):\n",
    "        s = env.reset()\n",
    "        for t in range(max_steps):\n",
    "            a = select_a_with_epsilon_greedy(s, Q, eps)\n",
    "            new_s, reward, done, info = env.step(a)\n",
    "            new_a = Q[new_s].argmax()\n",
    "            Q[s][a] += alpha * (reward + gamma * Q[new_s, new_a] - Q[s][a])\n",
    "            \n",
    "            history_length[i_episode] = t\n",
    "            history_reward[i_episode] += reward * gamma ** t\n",
    "            \n",
    "            s = new_s\n",
    "            \n",
    "            #eps *= eps_decay\n",
    "            if done:\n",
    "                # Running average of the terminal reward, which is used for controlling an exploration rate\n",
    "                # (This idea of controlling exploration rate by the terminal reward is suggested by JKCooper2)\n",
    "                # See https://gym.openai.com/evaluations/eval_xSOlwrBsQDqUW7y6lJOevQ\n",
    "                if avg_reward == None:\n",
    "                    avg_reward = reward\n",
    "                else:\n",
    "                    avg_reward = kappa * reward + (1 - kappa) * avg_reward\n",
    "                if reward > avg_reward:\n",
    "                    # Bias the current policy toward exploitation\n",
    "                    eps *= eps_decay\n",
    "\n",
    "            if done:\n",
    "                break\n",
    "    return get_greedy_policy(Q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 404,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-07-23 10:07:24,152] Making new env: FrozenLake8x8-v0\n"
     ]
    }
   ],
   "source": [
    "lake_env = gym.make('FrozenLake8x8-v0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 430,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/liza/anaconda2/lib/python2.7/site-packages/ipykernel/__main__.py:38: VisibleDeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n",
      "/home/liza/anaconda2/lib/python2.7/site-packages/ipykernel/__main__.py:39: VisibleDeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 21s, sys: 16 ms, total: 1min 21s\n",
      "Wall time: 1min 21s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "np.random.seed(42)\n",
    "lens, policy = dyna_q(lake_env, n_episodes = 1000, n_repeats=100,\n",
    "                    eps = 0.7, eps_decay = 0.995,\n",
    "                    alpha = 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 431,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 431,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "check_solution(lake_env, policy, n_episodes=1000, max_steps=250)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 415,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 14.3 s, sys: 0 ns, total: 14.3 s\n",
      "Wall time: 14.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "np.random.seed(42)\n",
    "policy = q_learning(lake_env, n_episodes = 20000, \n",
    "                    eps = 0.7, eps_decay = 0.995,\n",
    "                    alpha = 0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 419,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.95999999999999996"
      ]
     },
     "execution_count": 419,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "check_solution(lake_env, policy, n_episodes=1000, max_steps=250)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
