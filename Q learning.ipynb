{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from gym import wrappers\n",
    "def running_average(x, window_size, mode='valid'):\n",
    "    return np.convolve(x, np.ones(window_size) / window_size, mode=mode).max()\n",
    "\n",
    "def check_solution(env, policy, n_episodes = 100, max_steps = 100, to_wrap = False, to_send = False, name2save = ''):\n",
    "    ns = env.observation_space.n\n",
    "    na = env.action_space.n\n",
    "    count_dones = np.zeros(n_episodes)\n",
    "    count_steps = np.zeros(n_episodes)\n",
    "    if to_wrap:\n",
    "        env = wrappers.Monitor(env, name2save)\n",
    "        \n",
    "    for i in range(n_episodes):\n",
    "        observation = env.reset() \n",
    "        for step in range(max_steps): \n",
    "            action = policy[observation].argmax()\n",
    "            observation, reward, done, info = env.step(action)\n",
    "            count_dones[i] += reward\n",
    "            count_steps[i] += 1\n",
    "            if done:\n",
    "                break\n",
    "    \n",
    "\n",
    "    env.close()\n",
    "    if to_wrap and to_send:\n",
    "        gym.upload(name2save, api_key='sk_bExD4VfCSQukGlQkYKBhdQ')\n",
    "        \n",
    "    return running_average(count_dones, 100).max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def select_a_with_epsilon_greedy(curr_s, q_value, epsilon=0.1):\n",
    "    a = np.argmax(q_value[curr_s, :])\n",
    "    if np.random.rand() < epsilon:\n",
    "        a = np.random.randint(q_value.shape[1])\n",
    "    return a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_greedy_policy(Q):\n",
    "    ns, na = Q.shape\n",
    "    policy = np.zeros((ns, na))\n",
    "    best_actions = Q.argmax(axis = 1)\n",
    "    policy[np.arange(ns), best_actions] = 1 \n",
    "    return policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def q_learning(env, n_episodes = 1000, gamma = 0.999, alpha = 0.1, eps = 0.9, eps_decay = 0.995, kappa = 0.01):\n",
    "    max_steps = env.spec.tags.get('wrapper_config.TimeLimit.max_episode_steps')\n",
    "    \n",
    "    ns = env.observation_space.n\n",
    "    na = env.action_space.n\n",
    "    \n",
    "    Q = np.zeros((ns, na))\n",
    "    \n",
    "    history_length = np.zeros(n_episodes)\n",
    "    history_reward = np.zeros(n_episodes)\n",
    "    avg_reward = None\n",
    "    for i_episode in range(n_episodes):\n",
    "        s = env.reset()\n",
    "        for t in range(max_steps):\n",
    "            a = select_a_with_epsilon_greedy(s, Q, eps)\n",
    "            new_s, reward, done, info = env.step(a)\n",
    "            new_a = Q[new_s].argmax()\n",
    "            Q[s][a] += alpha * (reward + gamma * Q[new_s, new_a] - Q[s][a])\n",
    "            \n",
    "            history_length[i_episode] = t\n",
    "            history_reward[i_episode] += reward * gamma ** t\n",
    "            \n",
    "            s = new_s\n",
    "            \n",
    "            #eps *= eps_decay\n",
    "            if done:\n",
    "                # Running average of the terminal reward, which is used for controlling an exploration rate\n",
    "                # (This idea of controlling exploration rate by the terminal reward is suggested by JKCooper2)\n",
    "                # See https://gym.openai.com/evaluations/eval_xSOlwrBsQDqUW7y6lJOevQ\n",
    "                if avg_reward == None:\n",
    "                    avg_reward = reward\n",
    "                else:\n",
    "                    avg_reward = kappa * reward + (1 - kappa) * avg_reward\n",
    "                if reward > avg_reward:\n",
    "                    # Bias the current policy toward exploitation\n",
    "                    eps *= eps_decay\n",
    "\n",
    "            if done:\n",
    "                break\n",
    "    return get_greedy_policy(Q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import itertools\n",
    "def find_q_hyperparams(env):\n",
    "    alphas = [0.01, 0.1, 0.5, 0.8]\n",
    "    eps = [0.5, 0.7, 1.0]\n",
    "    kappas = [0.001, 0.01, 0.1]\n",
    "    n_episodes = [20000]\n",
    "    eps_decays = [1.0, 0.995]\n",
    "    \n",
    "    best_score = 0\n",
    "    history = []\n",
    "    for i,(alpha, eps,eps_decay, kappa, n) in enumerate(itertools.product(alphas, eps, eps_decays, kappas, n_episodes)):\n",
    "        policy = q_learning(env, n_episodes = n, eps = eps, eps_decay = eps_decay,  \n",
    "                              alpha = alpha, kappa = kappa)\n",
    "        score = check_solution(env,policy, n_episodes=250)\n",
    "        history.append((alpha, eps,eps_decay, kappa, n, score))\n",
    "        if score > best_score:\n",
    "            best_score = score\n",
    "            best_params = (alpha, eps,eps_decay, kappa, n)\n",
    "        print(i, score)\n",
    "    print(best_params)\n",
    "    return history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Frozen Lake 8x8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-07-03 16:52:49,825] Making new env: FrozenLake8x8-v0\n"
     ]
    }
   ],
   "source": [
    "lake_env = gym.make('FrozenLake8x8-v0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 13.9 s, sys: 0 ns, total: 13.9 s\n",
      "Wall time: 13.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "np.random.seed(42)\n",
    "policy = q_learning(lake_env, n_episodes = 20000, \n",
    "                    eps = 0.7, eps_decay = 0.995,\n",
    "                    alpha = 0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-07-03 16:53:36,018] Attempted to wrap env <FrozenLakeEnv instance> after .configure() was called. All wrappers must be applied before calling .configure()\n",
      "[2017-07-03 16:53:36,022] Creating monitor directory qlake-1\n",
      "[2017-07-03 16:53:36,025] Starting new video recorder writing to /home/liza/Документы/Python-projects/RL/qlake-1/openaigym.video.0.8332.video000000.json\n",
      "[2017-07-03 16:53:36,048] Starting new video recorder writing to /home/liza/Документы/Python-projects/RL/qlake-1/openaigym.video.0.8332.video000001.json\n",
      "[2017-07-03 16:53:36,082] Starting new video recorder writing to /home/liza/Документы/Python-projects/RL/qlake-1/openaigym.video.0.8332.video000008.json\n",
      "[2017-07-03 16:53:36,141] Starting new video recorder writing to /home/liza/Документы/Python-projects/RL/qlake-1/openaigym.video.0.8332.video000027.json\n",
      "[2017-07-03 16:53:36,236] Starting new video recorder writing to /home/liza/Документы/Python-projects/RL/qlake-1/openaigym.video.0.8332.video000064.json\n",
      "[2017-07-03 16:53:36,346] Starting new video recorder writing to /home/liza/Документы/Python-projects/RL/qlake-1/openaigym.video.0.8332.video000125.json\n",
      "[2017-07-03 16:53:36,504] Starting new video recorder writing to /home/liza/Документы/Python-projects/RL/qlake-1/openaigym.video.0.8332.video000216.json\n",
      "[2017-07-03 16:53:36,736] Starting new video recorder writing to /home/liza/Документы/Python-projects/RL/qlake-1/openaigym.video.0.8332.video000343.json\n",
      "[2017-07-03 16:53:37,013] Starting new video recorder writing to /home/liza/Документы/Python-projects/RL/qlake-1/openaigym.video.0.8332.video000512.json\n",
      "[2017-07-03 16:53:37,354] Starting new video recorder writing to /home/liza/Документы/Python-projects/RL/qlake-1/openaigym.video.0.8332.video000729.json\n",
      "[2017-07-03 16:53:37,790] Finished writing results. You can upload them to the scoreboard via gym.upload('/home/liza/\\xd0\\x94\\xd0\\xbe\\xd0\\xba\\xd1\\x83\\xd0\\xbc\\xd0\\xb5\\xd0\\xbd\\xd1\\x82\\xd1\\x8b/Python-projects/RL/qlake-1')\n",
      "[2017-07-03 16:53:37,793] [FrozenLake8x8-v0] Uploading 1000 episodes of training data\n",
      "[2017-07-03 16:53:40,639] [FrozenLake8x8-v0] Uploading videos of 10 training episodes (4003 bytes)\n",
      "[2017-07-03 16:53:41,238] [FrozenLake8x8-v0] Creating evaluation object from qlake-1 with learning curve and training video\n",
      "[2017-07-03 16:53:41,618] \n",
      "****************************************************\n",
      "You successfully uploaded your evaluation on FrozenLake8x8-v0 to\n",
      "OpenAI Gym! You can find it at:\n",
      "\n",
      "    https://gym.openai.com/evaluations/eval_a6PUwWsUSoiB1sCjXGNCw\n",
      "\n",
      "****************************************************\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.94999999999999996"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.RandomState(1)\n",
    "check_solution(lake_env, policy, n_episodes = 1000, max_steps = 250, to_wrap = True, to_send = True,\n",
    "               name2save='qlake-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.95999999999999996"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "check_solution(lake_env, policy, n_episodes = 1000, max_steps = 250)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-06-30 21:46:50,959] Making new env: Taxi-v1\n"
     ]
    }
   ],
   "source": [
    "taxi_env = gym.make('Taxi-v1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 13.2 s, sys: 0 ns, total: 13.2 s\n",
      "Wall time: 13.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "np.random.seed(42)\n",
    "taxi_policy = q_learning(taxi_env, n_episodes = 20000, \n",
    "                    eps = 0.7, eps_decay = 0.995,\n",
    "                    alpha = 0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10.81"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.RandomState(1)\n",
    "check_solution(taxi_env, taxi_policy, n_episodes = 1000, max_steps = 250)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
